{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file_path = \"configs/mutag_gnn_preds.json\"\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    settings = json.load(json_file)\n",
    "for str_target_concept, examples in settings.items():\n",
    "    positive_examples = set(examples[\"positive_examples\"])\n",
    "\n",
    "    negative_examples = set(examples[\"negative_examples\"])\n",
    "    print(len(positive_examples.intersection(negative_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file_path = \"configs/mutag.json\"\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    settings = json.load(json_file)\n",
    "for str_target_concept, examples in settings[\"problems\"].items():\n",
    "    positive_examples = set(examples[\"positive_examples\"])\n",
    "\n",
    "    negative_examples = set(examples[\"negative_examples\"])\n",
    "    print(len(positive_examples.intersection(negative_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from src.dglnn_local.subgraphx import HeteroSubgraphX\n",
    "from src.gnn_explainers.configs import get_configs\n",
    "from src.gnn_explainers.dataset import RDFDatasets\n",
    "from src.gnn_explainers.hetro_features import HeteroFeature\n",
    "from src.gnn_explainers.model import RGCN\n",
    "from src.gnn_explainers.trainer import train_gnn\n",
    "from src.gnn_explainers.utils import get_nodes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lp_mutag(gnn_pred_dt, idx_map):\n",
    "    positive_examples = [\n",
    "        idx_map[item][\"IRI\"] for item in gnn_pred_dt if gnn_pred_dt[item] == 1\n",
    "    ]\n",
    "    negative_examples = [\n",
    "        idx_map[item][\"IRI\"] for item in gnn_pred_dt if gnn_pred_dt[item] == 0\n",
    "    ]\n",
    "\n",
    "    lp_dict = {\n",
    "        \"carcino\": {\n",
    "            \"positive_examples\": positive_examples,\n",
    "            \"negative_examples\": negative_examples,\n",
    "        }\n",
    "    }\n",
    "    return lp_dict\n",
    "\n",
    "\n",
    "def get_lp_mutag_train_test(gnn_pred_dt_train, gnn_pred_dt_test, idx_map):\n",
    "    # Positive and negative examples for training set\n",
    "    train_positive_examples = [\n",
    "        idx_map[item][\"IRI\"]\n",
    "        for item in gnn_pred_dt_train\n",
    "        if gnn_pred_dt_train[item] == 1\n",
    "    ]\n",
    "    train_negative_examples = [\n",
    "        idx_map[item][\"IRI\"]\n",
    "        for item in gnn_pred_dt_train\n",
    "        if gnn_pred_dt_train[item] == 0\n",
    "    ]\n",
    "\n",
    "    # Positive and negative examples for test set\n",
    "    test_positive_examples = [\n",
    "        idx_map[item][\"IRI\"] for item in gnn_pred_dt_test if gnn_pred_dt_test[item] == 1\n",
    "    ]\n",
    "    test_negative_examples = [\n",
    "        idx_map[item][\"IRI\"] for item in gnn_pred_dt_test if gnn_pred_dt_test[item] == 0\n",
    "    ]\n",
    "\n",
    "    lp_dict_test_train = {\n",
    "        \"carcino\": {\n",
    "            \"positive_examples_train\": train_positive_examples,\n",
    "            \"negative_examples_train\": train_negative_examples,\n",
    "            \"positive_examples_test\": test_positive_examples,\n",
    "            \"negative_examples_test\": test_negative_examples,\n",
    "        }\n",
    "    }\n",
    "    return lp_dict_test_train\n",
    "\n",
    "\n",
    "def get_lp_aifb(gnn_pred_dt, idx_map):\n",
    "    class_to_pred = {}\n",
    "    multi_lp_dict = {}\n",
    "\n",
    "    # Creating new_dict\n",
    "    for key, value in gnn_pred_dt.items():\n",
    "        class_to_pred.setdefault(value, []).append(key)\n",
    "\n",
    "    # Creating news_dict\n",
    "    multi_lp_dict = {\n",
    "        f\"id{key+1}instance\": {\n",
    "            \"positive_examples\": [idx_map[val][\"IRI\"] for val in values],\n",
    "            \"negative_examples\": [\n",
    "                idx_map[val][\"IRI\"]\n",
    "                for k, v in class_to_pred.items()\n",
    "                if k != key\n",
    "                for val in v\n",
    "            ],\n",
    "        }\n",
    "        for key, values in class_to_pred.items()\n",
    "    }\n",
    "\n",
    "    return multi_lp_dict\n",
    "\n",
    "\n",
    "def get_lp_aifb_train_test(gnn_pred_dt_train, gnn_pred_dt_test, idx_map):\n",
    "    class_to_pred_train = {}\n",
    "    class_to_pred_test = {}\n",
    "\n",
    "    # Creating new_dict for training set\n",
    "    for key, value in gnn_pred_dt_train.items():\n",
    "        class_to_pred_train.setdefault(value, []).append(key)\n",
    "\n",
    "    # Creating new_dict for test set\n",
    "    for key, value in gnn_pred_dt_test.items():\n",
    "        class_to_pred_test.setdefault(value, []).append(key)\n",
    "\n",
    "    # Merge training and test dictionaries\n",
    "    all_class_to_pred = {**class_to_pred_train, **class_to_pred_test}\n",
    "\n",
    "    multi_lp_dict = {\n",
    "        f\"id{key+1}instance\": {\n",
    "            \"positive_examples_train\": [\n",
    "                idx_map[val][\"IRI\"] for val in class_to_pred_train.get(key, [])\n",
    "            ],\n",
    "            \"negative_examples_train\": [\n",
    "                idx_map[val][\"IRI\"]\n",
    "                for k, v in class_to_pred_train.items()\n",
    "                if k != key\n",
    "                for val in v\n",
    "            ],\n",
    "            \"positive_examples_test\": [\n",
    "                idx_map[val][\"IRI\"] for val in class_to_pred_test.get(key, [])\n",
    "            ],\n",
    "            \"negative_examples_test\": [\n",
    "                idx_map[val][\"IRI\"]\n",
    "                for k, v in class_to_pred_test.items()\n",
    "                if k != key\n",
    "                for val in v\n",
    "            ],\n",
    "        }\n",
    "        for key in all_class_to_pred.keys()\n",
    "    }\n",
    "\n",
    "    return multi_lp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = \"mutag\"\n",
    "dataset = str.lower(dataset)\n",
    "\n",
    "configs = get_configs(dataset)\n",
    "hidden_dim = configs[\"hidden_dim\"]\n",
    "num_bases = configs[\"n_bases\"]\n",
    "lr = configs[\"lr\"]\n",
    "weight_decay = configs[\"weight_decay\"]\n",
    "epochs = configs[\"max_epoch\"]\n",
    "validation = configs[\"validation\"]\n",
    "hidden_layers = configs[\"num_layers\"] - 1\n",
    "act = None\n",
    "\n",
    "my_dataset = RDFDatasets(dataset, root=\"data/\", validation=validation)\n",
    "g = my_dataset.g.to(device)\n",
    "out_dim = my_dataset.num_classes\n",
    "e_types = g.etypes\n",
    "category = my_dataset.category\n",
    "train_idx = my_dataset.train_idx.to(device)\n",
    "test_idx = my_dataset.test_idx.to(device)\n",
    "labels = my_dataset.labels.to(device)\n",
    "\n",
    "if validation:\n",
    "    valid_idx = my_dataset.valid_idx.to(device)\n",
    "    test_idx = torch.cat([test_idx, valid_idx], dim=0)\n",
    "\n",
    "idx_map = my_dataset.idx_map\n",
    "# pred_idx = torch.cat([train_idx, test_idx], dim=0)\n",
    "pred_idx = test_idx\n",
    "input_feature = HeteroFeature({}, get_nodes_dict(g), hidden_dim, act=act).to(device)\n",
    "model = RGCN(\n",
    "    hidden_dim,\n",
    "    hidden_dim,\n",
    "    out_dim,\n",
    "    e_types,\n",
    "    num_bases,\n",
    "    category,\n",
    ").to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "loss_fn = F.cross_entropy\n",
    "model.add_module(\"input_feature\", input_feature)\n",
    "optimizer.add_param_group({\"params\": input_feature.parameters()})\n",
    "\n",
    "PATH = f\"trained_models/{dataset}_trained.pt\"\n",
    "if not os.path.isfile(PATH):\n",
    "    train_gnn(dataset=dataset, PATH=PATH)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "epoch = checkpoint[\"epoch\"]\n",
    "loss = checkpoint[\"loss\"]\n",
    "feat = model.input_feature()\n",
    "\n",
    "test_idx = my_dataset.test_idx.to(device)\n",
    "labels = my_dataset.labels.to(device)\n",
    "gt = labels[test_idx].tolist()\n",
    "pred_logit = model(g, feat)[category]\n",
    "gnn_preds_test = pred_logit[test_idx].argmax(dim=1).tolist()\n",
    "gnn_preds_train = pred_logit[train_idx].argmax(dim=1).tolist()\n",
    "\n",
    "\n",
    "gnn_pred_dt_train = {\n",
    "    tensor.item(): pred for tensor, pred in zip(train_idx, gnn_preds_train)\n",
    "}\n",
    "gnn_pred_dt_test = {\n",
    "    tensor.item(): pred for tensor, pred in zip(train_idx, gnn_preds_test)\n",
    "}\n",
    "lp_data = get_lp_mutag(gnn_pred_dt_test, idx_map)\n",
    "lp_data_train_test = get_lp_mutag_train_test(\n",
    "    gnn_pred_dt_train, gnn_pred_dt_test, idx_map\n",
    ")\n",
    "# File path where you want to store the JSON data\n",
    "file_path = f\"configs/{dataset}_gnn_preds.json\"\n",
    "file_path_train_test = f\"configs/{dataset}_gnn_preds_train_test.json\"\n",
    "\n",
    "# Writing the dictionary to a JSON file with indentation\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(lp_data, json_file, indent=4)\n",
    "\n",
    "with open(file_path_train_test, \"w\") as json_file:\n",
    "    json.dump(lp_data_train_test, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id1instance\n",
      "60\n",
      "80\n",
      "id2instance\n",
      "48\n",
      "92\n",
      "id4instance\n",
      "12\n",
      "128\n",
      "id3instance\n",
      "20\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "for str_target_concept, examples in lp_data_train_test.items():\n",
    "    print(str_target_concept)\n",
    "    positive_examples = set(examples[\"positive_examples_train\"])\n",
    "    print(len(positive_examples))\n",
    "    negative_examples = set(examples[\"negative_examples_train\"])\n",
    "    print(len(negative_examples))\n",
    "    # print(len(positive_examples.intersection(negative_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(gnn_preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Problem created for Mutag dataset and stored at configs/mutag.json\n",
      "Learning Problem created for AIFB dataset and stored at configs/aifb.json\n"
     ]
    }
   ],
   "source": [
    "from src.utils.create_lp import create_lp_aifb, create_lp_mutag\n",
    "\n",
    "create_lp_mutag()\n",
    "create_lp_aifb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logical_explainers.EvoLearner import train_evo, train_evo_fid\n",
    "\n",
    "train_evo()\n",
    "train_evo_fid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
